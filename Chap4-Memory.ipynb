{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Memory: Enabling Your Chatbot to Learn from Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple version of this memory system using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in ./marketingvm/lib/python3.10/site-packages (0.0.38)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.59 (from langchain-community)\n",
      "  Using cached langchain_core-0.3.62-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.25 (from langchain-community)\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (3.12.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (0.1.147)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./marketingvm/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./marketingvm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./marketingvm/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./marketingvm/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./marketingvm/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./marketingvm/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./marketingvm/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./marketingvm/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./marketingvm/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./marketingvm/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./marketingvm/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./marketingvm/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: anyio in ./marketingvm/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./marketingvm/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./marketingvm/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in ./marketingvm/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: sniffio in ./marketingvm/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in ./marketingvm/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./marketingvm/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./marketingvm/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./marketingvm/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./marketingvm/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./marketingvm/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./marketingvm/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./marketingvm/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./marketingvm/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.0)\n",
      "Using cached langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.62-py3-none-any.whl (438 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: httpx-sse, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "\u001b[2K  Attempting uninstall: langchain-core\n",
      "\u001b[2K    Found existing installation: langchain-core 0.1.53\n",
      "\u001b[2K    Uninstalling langchain-core-0.1.53:\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.1.53\n",
      "\u001b[2K  Attempting uninstall: langchain-text-splitters━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain-text-splitters 0.0.2[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-text-splitters-0.0.2:━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-text-splitters-0.0.2━\u001b[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K  Attempting uninstall: langchain90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain 0.1.14━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-0.1.14:0m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-0.1.14━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langchain-core]\n",
      "\u001b[2K  Attempting uninstall: langchain-community╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [langchain]]\n",
      "\u001b[2K    Found existing installation: langchain-community 0.0.38━━━\u001b[0m \u001b[32m4/6\u001b[0m [langchain]\n",
      "\u001b[2K    Uninstalling langchain-community-0.0.38:\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [langchain]\n",
      "\u001b[2K      Successfully uninstalled langchain-community-0.0.3890m━━━━━━\u001b[0m \u001b[32m5/6\u001b[0m [langchain-community]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed httpx-sse-0.4.0 langchain-0.3.25 langchain-community-0.3.24 langchain-core-0.3.62 langchain-text-splitters-0.3.8 pydantic-settings-2.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.messages.utils import trim_messages, filter_messages\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from operator import itemgetter\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/3fk9wmx12gs74mp51cnpmhw80000gq/T/ipykernel_63851/1857497379.py:9: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:4b\")\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template\n",
    "prompt_temp = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Obtain a chat model\n",
    "# llm = Ollama(model=\"deepseek-r1:7b\")\n",
    "llm = Ollama(model=\"gemma3:4b\")\n",
    "\n",
    "\n",
    "# Create a simple chain\n",
    "chain = prompt_temp | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Gemma. I’m a large language model created by the Gemma team at Google DeepMind.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Invoke the chain.\n",
    "chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\",\"What is your name?\")\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is John.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Invoke the chain.\n",
    "# Note how the incorporation of the previous conversation in the chain enabled the model to answer the follow-up question in a context-aware manner.\n",
    "chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\",\"What is your name?\"),\n",
    "        (\"ai\", \"My name is John.\"),\n",
    "        (\"human\", \"Sorry. What is your name again?\"),\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst this may work for demo purposes, it won’t scale in a production environment because the list of conversation messages can grow significantly. Fortunately, LangChain provides a core utility class called *ChatMessageHistory*, which makes it easier to implement this memory system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='My name is John.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chat history object that can store messages in memory\n",
    "chat_history = InMemoryChatMessageHistory()\n",
    "\n",
    "# Add a user message to the chat history\n",
    "chat_history.add_user_message(\"What is your name?\")\n",
    "\n",
    "# Add an AI message to the chat history\n",
    "chat_history.add_ai_message(\"My name is John.\")\n",
    "\n",
    "# Print the chat history\n",
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is John.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can then integrate the stored chat messages into our chain and send a final prompt to the model\n",
    "response = chain.invoke({\n",
    "    \"messages\": chat_history.messages,\n",
    "})\n",
    "input = \"Sorry, what is your name again?\"\n",
    "chat_history.add_user_message(input)\n",
    "chain.invoke({\n",
    "    \"messages\": chat_history.messages,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we integrated the chat messages into the chain explicitly but this requires the tedious manual management of each new message. In a production setting, we need a way to persist chat history and automate the insertion and updating of it.\n",
    "\n",
    "To solve this problem, we can utilize LangChain’s RunnableWithMessageHistory class to automatically insert and update chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let’s modify our prompt template to incorporate a chat_history parameter which will later contain all prior chat messages\n",
    "prompt_temp = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "chain = prompt_temp | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let’s use the RunnableWithMessageHistory class to wrap our chain and incorporate the latest user input and chat history.\n",
    "chat_history_for_chain = InMemoryChatMessageHistory()\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    # Session_id is an identifier for the session (conversation) thread that the input messages correspond to. \n",
    "    # This allows you to maintain several conversations or threads with the same chain at the same time.\n",
    "    lambda session_id: chat_history_for_chain,\n",
    "    # An input_messages_key that specifies which part of the input should be tracked and stored in the chat history.\n",
    "    # In this example, we want to track the string passed in as input (match with the \"input\" key in the prompt).\n",
    "    input_messages_key=\"input\",\n",
    "    # A history_messages_key that specifies what the previous messages should be injected into the prompt as. \n",
    "    # Our prompt has a placeholder named \"history\", so we specify this property to match.\n",
    "    history_messages_key=\"history\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_for_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function <lambda> at 0x11a7f5240>, input_messages_key='input', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at an example where we return a chat history corresponding to each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain we used before\n",
    "prompt_temp = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "llm = Ollama(model=\"gemma3:4b\")\n",
    "chain = prompt_temp | llm\n",
    "\n",
    "# Keep track of the history for each combination of user_id and conversation_id.\n",
    "# Note: This line uses type hinting in Python 3.9+, which indicates that histories is a dictionary that will map session_id strings to chat history objects.\n",
    "# The code implies that when a new session starts, it can be stored in this dictionary like so: histories[session_id] = InMemoryChatMessageHistory()\n",
    "histories: dict[str, InMemoryChatMessageHistory] = {}\n",
    "\n",
    "# Define a function that takes a session_id as an argument and returns a chat history object.\n",
    "# Note: This line also uses type hinting in Python 3.9+. Denoting that session_id is a string, and the default value is an empty string.\n",
    "def get_session_history(session_id: str = ''):\n",
    "    if session_id not in histories:\n",
    "        histories[session_id] = InMemoryChatMessageHistory()\n",
    "    return histories[session_id]\n",
    "\n",
    "# Chain with history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. 😊 How’s your day going so far? Is there anything you’d like to chat about, or were you just saying hello?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In action by providing the input and session id\n",
    "with_message_history.invoke(\n",
    "    {\"input\": \"hi im bob!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob! 😄 It’s really nice to chat with you.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue the conversation\n",
    "with_message_history.invoke(\n",
    "    {\"input\": \"whats my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have no way of knowing your name! As an AI, I don't have access to personal information unless you explicitly tell me. 😊 \\n\\nCould you tell me your name?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New session_id --> does not remember\n",
    "with_message_history.invoke(\n",
    "    {\"input\": \"whats my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"456\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': InMemoryChatMessageHistory(messages=[HumanMessage(content='hi im bob!', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Bob! It's nice to meet you. 😊 How’s your day going so far? Is there anything you’d like to chat about, or were you just saying hello?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Bob! 😄 It’s really nice to chat with you.', additional_kwargs={}, response_metadata={})]),\n",
       " '456': InMemoryChatMessageHistory(messages=[HumanMessage(content='whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I have no way of knowing your name! As an AI, I don't have access to personal information unless you explicitly tell me. 😊 \\n\\nCould you tell me your name?\", additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can print the dictioinary\n",
    "histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Modify Chat History\n",
    "In many cases, the chat history messages aren’t in the best state or format to generate an accurate response from the model. To overcome this problem, we can modify the chat history in a variety of ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming messages​\n",
    "LLMs have limited context windows, therefore, the final prompt sent to the model can’t exceed the model’s input token limits. In addition, excessive prompt information can distract the model and lead to hallucination.\n",
    "\n",
    "An effective solution to this problem is to limit the number of messages retrieved from chat history and appended to the prompt. In practice, we need only to load and store the most recent chat n history messages. Let’s use an example chat history with some preloaded messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot use OPEN ai token counter, so lets configure a different one\n",
    "\n",
    "# # Define the LangChain's trim_messages function\n",
    "# trimmer = trim_messages(\n",
    "#     max_tokens=65,\n",
    "#     # Maintain the last messages \n",
    "#     strategy=\"last\",\n",
    "#     token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "#     # Include the system message\n",
    "#     include_system=True,\n",
    "#     # Do not allow partial messages\n",
    "#     allow_partial=False,\n",
    "#     # start_on=”human” ensures that we never remove an AIMessage (that is a response from the model) \n",
    "#     # without also removing corresponding HumanMessage (ie the question for that response).\n",
    "#     start_on=\"human\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE need to figure this piece out, that counts no of tokens.\n",
    "\n",
    "def dummy_token_counter(messages) -> int:\n",
    "    # treat each message like it adds 3 default tokens at the beginning\n",
    "    # of the message and at the end of the message. 3 + 4 + 3 = 10 tokens\n",
    "    # per message.\n",
    "\n",
    "    default_content_len = 4\n",
    "    default_msg_prefix_len = 3\n",
    "    default_msg_suffix_len = 3\n",
    "\n",
    "    count = 0\n",
    "    for msg in messages:\n",
    "        if isinstance(msg.content, str):\n",
    "            count += default_msg_prefix_len + default_content_len + default_msg_suffix_len\n",
    "        if isinstance(msg.content, list):\n",
    "            count += default_msg_prefix_len + len(msg.content) *  default_content_len + default_msg_suffix_len\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LangChain's trim_messages function\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    # Maintain the last messages \n",
    "    strategy=\"last\",\n",
    "    token_counter=dummy_token_counter,\n",
    "    # Include the system message\n",
    "    include_system=True,\n",
    "    # Do not allow partial messages\n",
    "    allow_partial=False,\n",
    "    # start_on=”human” ensures that we never remove an AIMessage (that is a response from the model) \n",
    "    # without also removing corresponding HumanMessage (ie the question for that response).\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a long message\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# Trim the message\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s incorporate the trimmer into a chain and RunnableWithMessageHistory. To use it in the chain, we need to ensure that the trimmer is run before the messages input to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt_temp = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Obtain an LLM\n",
    "llm = Ollama(model=\"gemma3:4b\")\n",
    "\n",
    "# This makes a \"messages\" key available to prompt template,\n",
    "# after passing the input messages list through the trimmer \n",
    "chain = {\"messages\": trimmer} | prompt_temp | llm\n",
    "\n",
    "# Tracking history\n",
    "history = InMemoryChatMessageHistory()\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    lambda: history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI: Why don’t scientists trust atoms? \\n\\n... Because they make up everything! \\n\\n😄 \\n\\nWould you like to hear another joke, or perhaps we could explore a different topic?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using it\n",
    "with_message_history.invoke(\n",
    "[HumanMessage(content=\"Today is a good day to learn about LangChain. Do you agree?\")]\n",
    ")\n",
    "with_message_history.invoke(\n",
    "[HumanMessage(content=\"Why is sky blue?\")]\n",
    ")\n",
    "with_message_history.invoke(\n",
    "[HumanMessage(content=\"What is the capital of France?\")]\n",
    ")\n",
    "with_message_history.invoke(\n",
    "[HumanMessage(content=\"Tell me a joke.\")]\n",
    ")\n",
    "with_message_history.invoke(\n",
    "[HumanMessage(content=\"What joke did you tell me? Could you repeat it?\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Today is a good day to learn about LangChain. Do you agree?', additional_kwargs={}, response_metadata={}), AIMessage(content='Absolutely! Today is a fantastic day to learn about LangChain. It’s a really exciting and rapidly evolving area of AI development, and there’s a huge amount of interesting stuff to discover. \\n\\nIt’s great that you’re taking the initiative to learn about it. \\n\\nTo help you get started, would you like me to:\\n\\n*   **Give you a brief overview of what LangChain is?** (A framework for building applications powered by language models)\\n*   **Suggest some resources for beginners?** (Tutorials, documentation, etc.)\\n*   **Answer a specific question you have about LangChain?** \\n\\nJust let me know where you’d like to start!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Why is sky blue?', additional_kwargs={}, response_metadata={}), AIMessage(content='That’s a fantastic question – it’s one that’s puzzled people for centuries! The short answer is due to a phenomenon called **Rayleigh scattering**. \\n\\nHere’s a breakdown of how it works:\\n\\n*   **Sunlight and Colors:** Sunlight is actually made up of *all* the colors of the rainbow.\\n*   **Entering the Atmosphere:** When sunlight enters the Earth’s atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).\\n*   **Scattering of Light:** This collision causes the light to scatter in different directions.\\n*   **Blue Light is Scattered More:** Blue and violet light have shorter wavelengths than other colors. Because of this, they are scattered *much* more strongly by these air molecules.  Think of it like bouncing a small ball (blue light) off a bumpy surface versus a larger ball (red light) - the smaller one will bounce around more randomly.\\n*   **Why not violet?** Violet light is scattered even *more* than blue, but our eyes are less sensitive to violet, and the sun emits slightly less violet light.\\n\\nSo, we see the sky as blue because blue light has been scattered across the atmosphere more than other colors.\\n\\nDoes that explanation make sense? Would you like me to delve deeper into any part of that explanation, such as:\\n\\n*   The relationship between wavelength and scattering?\\n*   Why sunsets are red or orange?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is **Paris**. \\n\\nIt’s a beautiful and historic city, full of iconic landmarks like the Eiffel Tower and the Louvre Museum! \\n\\nIs there anything else you’d like to know about Paris, or would you like to explore another aspect of LangChain?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke.', additional_kwargs={}, response_metadata={}), AIMessage(content='AI: Why don’t scientists trust atoms? \\n\\n... Because they make up everything! \\n\\n😄 \\n\\nWould you like to hear another joke, or perhaps we could explore a different topic?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What joke did you tell me? Could you repeat it?', additional_kwargs={}, response_metadata={}), AIMessage(content='AI: Why don’t scientists trust atoms? \\n\\n... Because they make up everything! \\n\\n😄 \\n\\nWould you like to hear another joke, or perhaps we could explore a different topic?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the history of the conversation\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary memory\n",
    "Aside from trimming messages, we can utilize the LLM to generate a summary of the conversation and then incorporate this summary into the prompt sent to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use ChatMessageHistory() to save the chat history\n",
    "# Note: while ChatMessageHistory serves as a base class for managing chat histories with potential for various storage implementations, \n",
    "# InMemoryChatMessageHistory is a concrete subclass that handles storage in memory.\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\",),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "# Create a chain that uses the prompt template\n",
    "chain = prompt | llm\n",
    "\n",
    "# Chain with history\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let’s create a function that will distill previous interactions into a summary. We can add this one to the front of the chain too.\n",
    "def summarize_messages(chain_input):\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) == 0:\n",
    "        return False\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    summarization_chain = summarization_prompt | llm\n",
    "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "    demo_ephemeral_chat_history.add_message(summary_message)\n",
    "    return True\n",
    "\n",
    "# Finally, we can add this function to the chain with the message history.\n",
    "chain_with_summarization = (\n",
    "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
    "    | chain_with_message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said your name was Nemo.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let’s invoke the chain and see if it remembers the chat history.\n",
    "chain_with_summarization.invoke(\n",
    "    {\"input\": \"What did I say my name was?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering messages\n",
    "As the list of chat history messages grows, a wider variety of types, sub-chains, and models may be utilized. LangChain provides a filter_messages helper that makes it easier to filter the chat history messages by type, id, or name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering messages\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
    "]\n",
    "filter_messages(messages, include_types=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}, id='1'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\n",
       " AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another filtering example\n",
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\n",
       " AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to filter messages\n",
    "filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filter_messages helper can also be used imperatively (as above) or declaratively (as below), \n",
    "# making it easy to compose with other components in a chain\n",
    "filter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "chain = filter_ | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat history with retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./marketingvm/lib/python3.10/site-packages (from pypdf) (4.13.2)\n",
      "Using cached pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\"/Users/hitesh.modi/Desktop/Kinda Personal/LLM Marketing Bot/pdf_files/Alex Hormozi 100 million leads.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def read_pdfs_into_pages(filepaths):\n",
    "    pages = []\n",
    "    for filepath in filepaths:\n",
    "        loader = PyPDFLoader(filepath)\n",
    "        async for page in loader.alazy_load():\n",
    "            pages.append(page)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = await read_pdfs_into_pages(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creator': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creationdate': '2023-08-21T11:14:02+00:00', 'author': 'Alex Hormozi', 'moddate': '2023-08-21T11:14:02+00:00', 'title': '$100M Leads: How to Get Strangers To Want To Buy Your Stuff', 'source': '/Users/hitesh.modi/Desktop/Kinda Personal/LLM Marketing Bot/pdf_files/Alex Hormozi 100 million leads.pdf', 'total_pages': 385, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creator': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creationdate': '2023-08-21T11:14:02+00:00', 'author': 'Alex Hormozi', 'moddate': '2023-08-21T11:14:02+00:00', 'title': '$100M Leads: How to Get Strangers To Want To Buy Your Stuff', 'source': '/Users/hitesh.modi/Desktop/Kinda Personal/LLM Marketing Bot/pdf_files/Alex Hormozi 100 million leads.pdf', 'total_pages': 385, 'page': 1, 'page_label': '2'}, page_content='Acquisition.com Volume II\\n \\n$100M Leads\\n \\nHow to Get Strangers To\\nWant To Buy Your Stuff\\n \\n \\nAlex Hormozi'),\n",
       " Document(metadata={'producer': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creator': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creationdate': '2023-08-21T11:14:02+00:00', 'author': 'Alex Hormozi', 'moddate': '2023-08-21T11:14:02+00:00', 'title': '$100M Leads: How to Get Strangers To Want To Buy Your Stuff', 'source': '/Users/hitesh.modi/Desktop/Kinda Personal/LLM Marketing Bot/pdf_files/Alex Hormozi 100 million leads.pdf', 'total_pages': 385, 'page': 2, 'page_label': '3'}, page_content='Copyright © 2023 by Alex Hormozi\\n \\nAll rights reserved. No part of this publication may be reproduced,\\ndistributed, or transmitted in any form or by any means, including\\nphotocopying, recording, or other electronic or mechanical\\nmethods, without the prior written permission of the publisher,\\nexcept in the case of brief questions embodied in critical reviews\\nand certain other noncommercial uses permitted by copyright law.\\nFor permission requests, write to the publisher at the address\\nbelow.\\nAcquisition.com\\n7710 N FM 620, Building 13C, Suite 100,\\nAustin, Texas 78726'),\n",
       " Document(metadata={'producer': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creator': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creationdate': '2023-08-21T11:14:02+00:00', 'author': 'Alex Hormozi', 'moddate': '2023-08-21T11:14:02+00:00', 'title': '$100M Leads: How to Get Strangers To Want To Buy Your Stuff', 'source': '/Users/hitesh.modi/Desktop/Kinda Personal/LLM Marketing Bot/pdf_files/Alex Hormozi 100 million leads.pdf', 'total_pages': 385, 'page': 3, 'page_label': '4'}, page_content='Guiding Principles\\n \\n \\n \\nDo more.'),\n",
       " Document(metadata={'producer': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creator': 'calibre (4.99.5) [https://calibre-ebook.com]', 'creationdate': '2023-08-21T11:14:02+00:00', 'author': 'Alex Hormozi', 'moddate': '2023-08-21T11:14:02+00:00', 'title': '$100M Leads: How to Get Strangers To Want To Buy Your Stuff', 'source': '/Users/hitesh.modi/Desktop/Kinda Personal/LLM Marketing Bot/pdf_files/Alex Hormozi 100 million leads.pdf', 'total_pages': 385, 'page': 4, 'page_label': '5'}, page_content='Thank Yous\\n \\n \\nTo Trevor:\\nThank you for your true friendship. Thank you for your tireless\\neffort to extract the ideas out of my head. And, for your continued\\nsupport in slaying the nihilism monster. People say you are lucky if\\nyou have one real friend in your entire life. Thank you for being\\nthe best friend a man could ask for.\\n \\nTo Leila:\\nEven though Lady Gaga said it first, it doesn’t make it any less\\ntrue.\\n“You found the light in me that I couldn’t find.\\nThe part of me that’s you will never die.”')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in ./marketingvm/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.7.0-cp310-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in ./marketingvm/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./marketingvm/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./marketingvm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./marketingvm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./marketingvm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./marketingvm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./marketingvm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./marketingvm/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./marketingvm/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./marketingvm/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./marketingvm/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./marketingvm/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./marketingvm/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./marketingvm/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "Using cached huggingface_hub-0.32.2-py3-none-any.whl (509 kB)\n",
      "Using cached hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached torch-2.7.0-cp310-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, networkx, hf-xet, fsspec, filelock, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [sympy]"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/3fk9wmx12gs74mp51cnpmhw80000gq/T/ipykernel_63851/1002843852.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(\n",
      "/Users/hitesh.modi/Desktop/Kinda Personal/LLM Marketing Bot/marketingvm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the document \n",
    "# loader = TextLoader(\"TeachingwithGenerativeAI.txt\")\n",
    "# doc = loader.load()\n",
    "\n",
    "## Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "## Define the embedding model\n",
    "embed_model = embedding\n",
    "\n",
    "# Create the vector store\n",
    "vector_db = FAISS.from_documents(\n",
    "    documents = chunks, \n",
    "    embedding = embed_model)\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s define a sub-chain that takes historical chat messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information. We’ll then use this sub-chain inside the final RAG chain, which will, in order,\n",
    "\n",
    "1. Rephrase the user’s question given the conversation history (if there is history)\n",
    "\n",
    "2. Pass the rephrased question to the retriever (see above) to get the most relevant documents\n",
    "\n",
    "3. Pass the original question, chat history and documents to the final prompt to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the content of a message\n",
    "def get_msg_content(msg):\n",
    "    # return msg.content\n",
    "    return msg\n",
    "\n",
    "# Define the SYSTEM prompt for contextualizing the chat history to come up with a standalone question\n",
    "contextualize_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Define the prompt for contextualizing the chat history to come up with a standalone question\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_system_prompt),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Define the chain for contextualizing the chat history to come up with a standalone question\n",
    "contextualize_chain = (\n",
    "    contextualize_prompt\n",
    "    | llm\n",
    "    | get_msg_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the question-answering SYSTEM prompt to generate the final answer\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Define the question-answering prompt to generate the final answer\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the chain to generate the final answer\n",
    "qa_chain = (\n",
    "    qa_prompt\n",
    "    | llm\n",
    "    | get_msg_content\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have made lots of chains above, we need to use the langchain one\n",
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the overall chain the uses both the retrieved documents and the chat history to answer the question\n",
    "@chain\n",
    "def history_aware_qa(input):\n",
    "     # Rephrase the question if needed\n",
    "     if input.get('chat_history'):\n",
    "         question = contextualize_chain.invoke(input)\n",
    "     else:\n",
    "         question = input['input']\n",
    "    \n",
    "     # Get context from the retriever\n",
    "     context = retriever.invoke(question)\n",
    "\n",
    "     # Get the final answer\n",
    "     return qa_chain.invoke({\n",
    "         **input,\n",
    "         \"context\": context\n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let’s incorporate stateful management of chat history and send the final prompt, \n",
    "# including chat history and retrieved context to the model for an output.\n",
    "chat_history_for_chain = InMemoryChatMessageHistory()\n",
    "qa_with_history = RunnableWithMessageHistory(\n",
    "    history_aware_qa,\n",
    "    lambda _: chat_history_for_chain,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer this question based on the provided context. The documents focus on training principles and automation strategies within a business context, not academic policies regarding AI use.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, let's invoke the chain\n",
    "qa_with_history.invoke(\n",
    "    {\"input\": \"Should faculty declare their AI policy in their classes?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You just asked: “Should faculty declare their AI policy in their classes?”'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try ask a related question that refers to the previous question\n",
    "qa_with_history.invoke(\n",
    "    {\"input\": \"What question did I just ask?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer this question. The provided documents discuss business strategies and training, not team size or personnel details.\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try ask a related question that refers to the previous question\n",
    "qa_with_history.invoke(\n",
    "    {\"input\": \"For your information, I would like to tell you that there are 1500 people in the team shaturbhatur\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer this question based on the provided context. The documents focus on training principles and automation strategies within a business context, not team size or personnel details.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try ask a related question that refers to the previous question\n",
    "qa_with_history.invoke(\n",
    "    {\"input\": \"how many people are there in the team shaturbhatur?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marketingvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
