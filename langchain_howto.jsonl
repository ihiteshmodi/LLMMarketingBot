{"title": "How to use tools in a chain", "url": "https://python.langchain.com/docs/how_to/tools_chain/", "sections": [{"heading": "Setup​", "text": "We'll need to install the following packages for this guide: If you'd like to trace your runs in LangSmith uncomment and set the following environment variables:", "code_blocks": ["%\npip install \n-\n-\nupgrade \n-\n-\nquiet langchain", "import\n getpass\nimport\n os\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"]}, {"heading": "Create a tool​", "text": "First, we need to create a tool to call. For this example, we will create a custom tool from a function. For more information on creating custom tools, please see this guide.", "code_blocks": ["from\n langchain_core\n.\ntools \nimport\n tool\n@tool\ndef\n \nmultiply\n(\nfirst_int\n:\n \nint\n,\n second_int\n:\n \nint\n)\n \n-\n>\n \nint\n:\n    \n\"\"\"Multiply two integers together.\"\"\"\n    \nreturn\n first_int \n*\n second_int", "print\n(\nmultiply\n.\nname\n)\nprint\n(\nmultiply\n.\ndescription\n)\nprint\n(\nmultiply\n.\nargs\n)", "multiply\nMultiply two integers together.\n{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}", "multiply\n.\ninvoke\n(\n{\n\"first_int\"\n:\n \n4\n,\n \n\"second_int\"\n:\n \n5\n}\n)", "20"]}, {"heading": "Chains​", "text": "If we know that we only need to use a tool a fixed number of times, we can create a chain for doing so. Let's create a simple chain that just multiplies user-specified numbers.  One of the most reliable ways to use tools with LLMs is with tool calling APIs (also sometimes called function calling). This only works with models that explicitly support tool calling. You can see which models support tool calling here, and learn more about how to use tool calling in this guide. First we'll define our model and tools. We'll start with just a single tool, multiply. We'll use bind_tools to pass the definition of our tool in as part of each call to the model, so that the model can invoke the tool when appropriate: When the model invokes the tool, this will show up in the AIMessage.tool_calls attribute of the output: Check out the LangSmith trace here. Great! We're able to generate tool invocations. But what if we want to actually call the tool? To do so we'll need to pass the generated tool args to our tool. As a simple example we'll just extract the arguments of the first tool_call: Check out the LangSmith trace here.", "code_blocks": ["pip install -qU \"langchain[openai]\"", "import\n getpass\nimport\n os\nif\n \nnot\n os\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\n  os\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n \n=\n getpass\n.\ngetpass\n(\n\"Enter API key for OpenAI: \"\n)\nfrom\n langchain\n.\nchat_models \nimport\n init_chat_model\nllm \n=\n init_chat_model\n(\n\"gpt-4o-mini\"\n,\n model_provider\n=\n\"openai\"\n)", "llm_with_tools \n=\n llm\n.\nbind_tools\n(\n[\nmultiply\n]\n)", "msg \n=\n llm_with_tools\n.\ninvoke\n(\n\"whats 5 times forty two\"\n)\nmsg\n.\ntool_calls", "[{'name': 'multiply',\n  'args': {'first_int': 5, 'second_int': 42},\n  'id': 'call_8QIg4QVFVAEeC1orWAgB2036',\n  'type': 'tool_call'}]", "from\n operator \nimport\n itemgetter\nchain \n=\n llm_with_tools \n|\n \n(\nlambda\n x\n:\n x\n.\ntool_calls\n[\n0\n]\n[\n\"args\"\n]\n)\n \n|\n multiply\nchain\n.\ninvoke\n(\n\"What's four times 23\"\n)", "92"]}, {"heading": "Agents​", "question": "What is covered in the section 'Agents​'?", "answer": "Chains are great when we know the specific sequence of tool usage needed for any user input. But for certain use cases, how many times we use tools depends on the input. In these cases, we want to let the model itself decide how many times to use tools and in what order. Agents let us do just this. We'll demonstrate a simple example using a LangGraph agent. See this tutorial for more detail.  Agents are also great because they make it easy to use multiple tools. With an agent, we can ask questions that require arbitrarily-many uses of our tools: Check out the LangSmith trace here.\n\n!pip install \n-\nqU langgraph\n\nfrom\n langgraph\n.\nprebuilt \nimport\n create_react_agent\n\n@tool\ndef\n \nadd\n(\nfirst_int\n:\n \nint\n,\n second_int\n:\n \nint\n)\n \n-\n>\n \nint\n:\n    \n\"Add two integers.\"\n    \nreturn\n first_int \n+\n second_int\n@tool\ndef\n \nexponentiate\n(\nbase\n:\n \nint\n,\n exponent\n:\n \nint\n)\n \n-\n>\n \nint\n:\n    \n\"Exponentiate the base to the exponent power.\"\n    \nreturn\n base\n**\nexponent\ntools \n=\n \n[\nmultiply\n,\n add\n,\n exponentiate\n]\n\n# Construct the tool calling agent\nagent \n=\n create_react_agent\n(\nllm\n,\n tools\n)\n\n# Use the agent\nquery \n=\n \n(\n    \n\"Take 3 to the fifth power and multiply that by the sum of twelve and \"\n    \n\"three, then square the whole result.\"\n)\ninput_message \n=\n \n{\n\"role\"\n:\n \n\"user\"\n,\n \n\"content\"\n:\n query\n}\nfor\n step \nin\n agent\n.\nstream\n(\n{\n\"messages\"\n:\n \n[\ninput_message\n]\n}\n,\n stream_mode\n=\n\"values\"\n)\n:\n    step\n[\n\"messages\"\n]\n[\n-\n1\n]\n.\npretty_print\n(\n)\n\n================================\u001b[1m Human Message \u001b[0m=================================\nTake 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result.\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\n  exponentiate (call_EHGS8gnEVNCJQ9rVOk11KCQH)\n Call ID: call_EHGS8gnEVNCJQ9rVOk11KCQH\n  Args:\n    base: 3\n    exponent: 5\n  add (call_s2cxOrXEKqI6z7LWbMUG6s8c)\n Call ID: call_s2cxOrXEKqI6z7LWbMUG6s8c\n  Args:\n    first_int: 12\n    second_int: 3\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: add\n15\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\n  multiply (call_25v5JEfDWuKNgmVoGBan0d7J)\n Call ID: call_25v5JEfDWuKNgmVoGBan0d7J\n  Args:\n    first_int: 243\n    second_int: 15\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: multiply\n3645\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\n  exponentiate (call_x1yKEeBPrFYmCp2z5Kn8705r)\n Call ID: call_x1yKEeBPrFYmCp2z5Kn8705r\n  Args:\n    base: 3645\n    exponent: 2\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: exponentiate\n13286025\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe final result of taking 3 to the fifth power, multiplying it by the sum of twelve and three, and then squaring the whole result is **13,286,025**."}]}
{"title": "How to use a vectorstore as a retriever", "url": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "sections": [{"heading": "Creating a retriever from a vectorstore​", "text": "You can build a retriever from a vectorstore using its .as_retriever method. Let's walk through an example. First we instantiate a vectorstore. We will use an in-memory FAISS vectorstore: We can then instantiate a retriever: This creates a retriever (specifically a VectorStoreRetriever), which we can use in the usual way:", "code_blocks": ["from\n langchain_community\n.\ndocument_loaders \nimport\n TextLoader\nfrom\n langchain_community\n.\nvectorstores \nimport\n FAISS\nfrom\n langchain_openai \nimport\n OpenAIEmbeddings\nfrom\n langchain_text_splitters \nimport\n CharacterTextSplitter\nloader \n=\n TextLoader\n(\n\"state_of_the_union.txt\"\n)\ndocuments \n=\n loader\n.\nload\n(\n)\ntext_splitter \n=\n CharacterTextSplitter\n(\nchunk_size\n=\n1000\n,\n chunk_overlap\n=\n0\n)\ntexts \n=\n text_splitter\n.\nsplit_documents\n(\ndocuments\n)\nembeddings \n=\n OpenAIEmbeddings\n(\n)\nvectorstore \n=\n FAISS\n.\nfrom_documents\n(\ntexts\n,\n embeddings\n)", "retriever \n=\n vectorstore\n.\nas_retriever\n(\n)", "docs \n=\n retriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)"]}, {"heading": "Maximum marginal relevance retrieval​", "text": "By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. This effectively specifies what method on the underlying vectorstore is used (e.g., similarity_search, max_marginal_relevance_search, etc.).", "code_blocks": ["retriever \n=\n vectorstore\n.\nas_retriever\n(\nsearch_type\n=\n\"mmr\"\n)", "docs \n=\n retriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)"]}, {"heading": "Passing search parameters​", "question": "What is covered in the section 'Passing search parameters​'?", "answer": "We can pass parameters to the underlying vectorstore's search methods using search_kwargs. For example, we can set a similarity score threshold and only return documents with a score above that threshold. We can also limit the number of documents k returned by the retriever.\n\nretriever \n=\n vectorstore\n.\nas_retriever\n(\n    search_type\n=\n\"similarity_score_threshold\"\n,\n search_kwargs\n=\n{\n\"score_threshold\"\n:\n \n0.5\n}\n)\n\ndocs \n=\n retriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)\n\nretriever \n=\n vectorstore\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n \n1\n}\n)\n\ndocs \n=\n retriever\n.\ninvoke\n(\n\"what did the president say about ketanji brown jackson?\"\n)\nlen\n(\ndocs\n)\n\n1"}]}
{"title": "How to add memory to chatbots", "url": "https://python.langchain.com/docs/how_to/chatbots_memory/", "sections": [{"heading": "Setup​", "text": "You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY: Let's also set up a chat model that we'll use for the below examples.", "code_blocks": ["%\npip install \n-\n-\nupgrade \n-\n-\nquiet langchain langchain\n-\nopenai langgraph\nimport\n getpass\nimport\n os\nif\n \nnot\n os\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n:\n    os\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n \n=\n getpass\n.\ngetpass\n(\n\"OpenAI API Key:\"\n)", "OpenAI API Key: ········", "from\n langchain_openai \nimport\n ChatOpenAI\nmodel \n=\n ChatOpenAI\n(\nmodel\n=\n\"gpt-4o-mini\"\n)"]}, {"heading": "Message passing​", "text": "The simplest form of memory is simply passing chat history messages into a chain. Here's an example: We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.", "code_blocks": ["from\n langchain_core\n.\nmessages \nimport\n AIMessage\n,\n HumanMessage\n,\n SystemMessage\nfrom\n langchain_core\n.\nprompts \nimport\n ChatPromptTemplate\n,\n MessagesPlaceholder\nprompt \n=\n ChatPromptTemplate\n.\nfrom_messages\n(\n    \n[\n        SystemMessage\n(\n            content\n=\n\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        \n)\n,\n        MessagesPlaceholder\n(\nvariable_name\n=\n\"messages\"\n)\n,\n    \n]\n)\nchain \n=\n prompt \n|\n model\nai_msg \n=\n chain\n.\ninvoke\n(\n    \n{\n        \n\"messages\"\n:\n \n[\n            HumanMessage\n(\n                content\n=\n\"Translate from English to French: I love programming.\"\n            \n)\n,\n            AIMessage\n(\ncontent\n=\n\"J'adore la programmation.\"\n)\n,\n            HumanMessage\n(\ncontent\n=\n\"What did you just say?\"\n)\n,\n        \n]\n,\n    \n}\n)\nprint\n(\nai_msg\n.\ncontent\n)", "I said, \"I love programming\" in French: \"J'adore la programmation.\""]}, {"heading": "Automatic history management​", "text": "The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's persistence. You can enable persistence in LangGraph applications by providing a checkpointer when compiling the graph. We'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:", "code_blocks": ["from\n langgraph\n.\ncheckpoint\n.\nmemory \nimport\n MemorySaver\nfrom\n langgraph\n.\ngraph \nimport\n START\n,\n MessagesState\n,\n StateGraph\nworkflow \n=\n StateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\n \ncall_model\n(\nstate\n:\n MessagesState\n)\n:\n    system_prompt \n=\n \n(\n        \n\"You are a helpful assistant. \"\n        \n\"Answer all questions to the best of your ability.\"\n    \n)\n    messages \n=\n \n[\nSystemMessage\n(\ncontent\n=\nsystem_prompt\n)\n]\n \n+\n state\n[\n\"messages\"\n]\n    response \n=\n model\n.\ninvoke\n(\nmessages\n)\n    \nreturn\n \n{\n\"messages\"\n:\n response\n}\n# Define the node and edge\nworkflow\n.\nadd_node\n(\n\"model\"\n,\n call_model\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n \n\"model\"\n)\n# Add simple in-memory checkpointer\nmemory \n=\n MemorySaver\n(\n)\napp \n=\n workflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)", "app\n.\ninvoke\n(\n    \n{\n\"messages\"\n:\n \n[\nHumanMessage\n(\ncontent\n=\n\"Translate to French: I love programming.\"\n)\n]\n}\n,\n    config\n=\n{\n\"configurable\"\n:\n \n{\n\"thread_id\"\n:\n \n\"1\"\n}\n}\n,\n)", "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}", "app\n.\ninvoke\n(\n    \n{\n\"messages\"\n:\n \n[\nHumanMessage\n(\ncontent\n=\n\"What did I just ask you?\"\n)\n]\n}\n,\n    config\n=\n{\n\"configurable\"\n:\n \n{\n\"thread_id\"\n:\n \n\"1\"\n}\n}\n,\n)", "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),\n  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),\n  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}"]}, {"heading": "Modifying chat history​", "question": "What is covered in the section 'Modifying chat history​'?", "answer": "Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples: LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the app we declared above: We can see the app remembers the preloaded name. But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in trim_messages util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages: Let's call this new app and check the response We can see that trim_messages was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it. Check out our how to guide on trimming messages for more. We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history: And now, let's update the model-calling function to distill previous interactions into a summary: Let's see if it remembers the name we gave it: Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\n\ndemo_ephemeral_chat_history \n=\n \n[\n    HumanMessage\n(\ncontent\n=\n\"Hey there! I'm Nemo.\"\n)\n,\n    AIMessage\n(\ncontent\n=\n\"Hello!\"\n)\n,\n    HumanMessage\n(\ncontent\n=\n\"How are you today?\"\n)\n,\n    AIMessage\n(\ncontent\n=\n\"Fine thanks!\"\n)\n,\n]\napp\n.\ninvoke\n(\n    \n{\n        \n\"messages\"\n:\n demo_ephemeral_chat_history\n        \n+\n \n[\nHumanMessage\n(\ncontent\n=\n\"What's my name?\"\n)\n]\n    \n}\n,\n    config\n=\n{\n\"configurable\"\n:\n \n{\n\"thread_id\"\n:\n \n\"2\"\n}\n}\n,\n)\n\n{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),\n  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}\n\nfrom\n langchain_core\n.\nmessages \nimport\n trim_messages\nfrom\n langgraph\n.\ncheckpoint\n.\nmemory \nimport\n MemorySaver\nfrom\n langgraph\n.\ngraph \nimport\n START\n,\n MessagesState\n,\n StateGraph\n# Define trimmer\n# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\ntrimmer \n=\n trim_messages\n(\nstrategy\n=\n\"last\"\n,\n max_tokens\n=\n2\n,\n token_counter\n=\nlen\n)\nworkflow \n=\n StateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\n \ncall_model\n(\nstate\n:\n MessagesState\n)\n:\n    trimmed_messages \n=\n trimmer\n.\ninvoke\n(\nstate\n[\n\"messages\"\n]\n)\n    system_prompt \n=\n \n(\n        \n\"You are a helpful assistant. \"\n        \n\"Answer all questions to the best of your ability.\"\n    \n)\n    messages \n=\n \n[\nSystemMessage\n(\ncontent\n=\nsystem_prompt\n)\n]\n \n+\n trimmed_messages\n    response \n=\n model\n.\ninvoke\n(\nmessages\n)\n    \nreturn\n \n{\n\"messages\"\n:\n response\n}\n# Define the node and edge\nworkflow\n.\nadd_node\n(\n\"model\"\n,\n call_model\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n \n\"model\"\n)\n# Add simple in-memory checkpointer\nmemory \n=\n MemorySaver\n(\n)\napp \n=\n workflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\n\napp\n.\ninvoke\n(\n    \n{\n        \n\"messages\"\n:\n demo_ephemeral_chat_history\n        \n+\n \n[\nHumanMessage\n(\ncontent\n=\n\"What is my name?\"\n)\n]\n    \n}\n,\n    config\n=\n{\n\"configurable\"\n:\n \n{\n\"thread_id\"\n:\n \n\"3\"\n}\n}\n,\n)\n\n{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),\n  AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}\n\ndemo_ephemeral_chat_history \n=\n \n[\n    HumanMessage\n(\ncontent\n=\n\"Hey there! I'm Nemo.\"\n)\n,\n    AIMessage\n(\ncontent\n=\n\"Hello!\"\n)\n,\n    HumanMessage\n(\ncontent\n=\n\"How are you today?\"\n)\n,\n    AIMessage\n(\ncontent\n=\n\"Fine thanks!\"\n)\n,\n]\n\nfrom\n langchain_core\n.\nmessages \nimport\n HumanMessage\n,\n RemoveMessage\nfrom\n langgraph\n.\ncheckpoint\n.\nmemory \nimport\n MemorySaver\nfrom\n langgraph\n.\ngraph \nimport\n START\n,\n MessagesState\n,\n StateGraph\nworkflow \n=\n StateGraph\n(\nstate_schema\n=\nMessagesState\n)\n# Define the function that calls the model\ndef\n \ncall_model\n(\nstate\n:\n MessagesState\n)\n:\n    system_prompt \n=\n \n(\n        \n\"You are a helpful assistant. \"\n        \n\"Answer all questions to the best of your ability. \"\n        \n\"The provided chat history includes a summary of the earlier conversation.\"\n    \n)\n    system_message \n=\n SystemMessage\n(\ncontent\n=\nsystem_prompt\n)\n    message_history \n=\n state\n[\n\"messages\"\n]\n[\n:\n-\n1\n]\n  \n# exclude the most recent user input\n    \n# Summarize the messages if the chat history reaches a certain size\n    \nif\n \nlen\n(\nmessage_history\n)\n \n>=\n \n4\n:\n        last_human_message \n=\n state\n[\n\"messages\"\n]\n[\n-\n1\n]\n        \n# Invoke the model to generate conversation summary\n        summary_prompt \n=\n \n(\n            \n\"Distill the above chat messages into a single summary message. \"\n            \n\"Include as many specific details as you can.\"\n        \n)\n        summary_message \n=\n model\n.\ninvoke\n(\n            message_history \n+\n \n[\nHumanMessage\n(\ncontent\n=\nsummary_prompt\n)\n]\n        \n)\n        \n# Delete messages that we no longer want to show up\n        delete_messages \n=\n \n[\nRemoveMessage\n(\nid\n=\nm\n.\nid\n)\n \nfor\n m \nin\n state\n[\n\"messages\"\n]\n]\n        \n# Re-add user message\n        human_message \n=\n HumanMessage\n(\ncontent\n=\nlast_human_message\n.\ncontent\n)\n        \n# Call the model with summary & response\n        response \n=\n model\n.\ninvoke\n(\n[\nsystem_message\n,\n summary_message\n,\n human_message\n]\n)\n        message_updates \n=\n \n[\nsummary_message\n,\n human_message\n,\n response\n]\n \n+\n delete_messages\n    \nelse\n:\n        message_updates \n=\n model\n.\ninvoke\n(\n[\nsystem_message\n]\n \n+\n state\n[\n\"messages\"\n]\n)\n    \nreturn\n \n{\n\"messages\"\n:\n message_updates\n}\n# Define the node and edge\nworkflow\n.\nadd_node\n(\n\"model\"\n,\n call_model\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n \n\"model\"\n)\n# Add simple in-memory checkpointer\nmemory \n=\n MemorySaver\n(\n)\napp \n=\n workflow\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\n\napp\n.\ninvoke\n(\n    \n{\n        \n\"messages\"\n:\n demo_ephemeral_chat_history\n        \n+\n \n[\nHumanMessage\n(\n\"What did I say my name was?\"\n)\n]\n    \n}\n,\n    config\n=\n{\n\"configurable\"\n:\n \n{\n\"thread_id\"\n:\n \n\"4\"\n}\n}\n,\n)\n\n{'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),\n  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),\n  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}"}]}
{"title": "How to use example selectors", "url": "https://python.langchain.com/docs/how_to/example_selectors/", "sections": [{"heading": "Examples​", "text": "In order to use an example selector, we need to create a list of examples. These should generally be example inputs and outputs. For this demo purpose, let's imagine we are selecting examples of how to translate English to Italian.", "code_blocks": ["examples \n=\n \n[\n    \n{\n\"input\"\n:\n \n\"hi\"\n,\n \n\"output\"\n:\n \n\"ciao\"\n}\n,\n    \n{\n\"input\"\n:\n \n\"bye\"\n,\n \n\"output\"\n:\n \n\"arrivederci\"\n}\n,\n    \n{\n\"input\"\n:\n \n\"soccer\"\n,\n \n\"output\"\n:\n \n\"calcio\"\n}\n,\n]"]}, {"heading": "Custom Example Selector​", "text": "Let's write an example selector that chooses what example to pick based on the length of the word.", "code_blocks": ["from\n langchain_core\n.\nexample_selectors\n.\nbase \nimport\n BaseExampleSelector\nclass\n \nCustomExampleSelector\n(\nBaseExampleSelector\n)\n:\n    \ndef\n \n__init__\n(\nself\n,\n examples\n)\n:\n        self\n.\nexamples \n=\n examples\n    \ndef\n \nadd_example\n(\nself\n,\n example\n)\n:\n        self\n.\nexamples\n.\nappend\n(\nexample\n)\n    \ndef\n \nselect_examples\n(\nself\n,\n input_variables\n)\n:\n        \n# This assumes knowledge that part of the input will be a 'text' key\n        new_word \n=\n input_variables\n[\n\"input\"\n]\n        new_word_length \n=\n \nlen\n(\nnew_word\n)\n        \n# Initialize variables to store the best match and its length difference\n        best_match \n=\n \nNone\n        smallest_diff \n=\n \nfloat\n(\n\"inf\"\n)\n        \n# Iterate through each example\n        \nfor\n example \nin\n self\n.\nexamples\n:\n            \n# Calculate the length difference with the first word of the example\n            current_diff \n=\n \nabs\n(\nlen\n(\nexample\n[\n\"input\"\n]\n)\n \n-\n new_word_length\n)\n            \n# Update the best match if the current one is closer in length\n            \nif\n current_diff \n<\n smallest_diff\n:\n                smallest_diff \n=\n current_diff\n                best_match \n=\n example\n        \nreturn\n \n[\nbest_match\n]", "example_selector \n=\n CustomExampleSelector\n(\nexamples\n)", "example_selector\n.\nselect_examples\n(\n{\n\"input\"\n:\n \n\"okay\"\n}\n)", "[{'input': 'bye', 'output': 'arrivederci'}]", "example_selector\n.\nadd_example\n(\n{\n\"input\"\n:\n \n\"hand\"\n,\n \n\"output\"\n:\n \n\"mano\"\n}\n)", "example_selector\n.\nselect_examples\n(\n{\n\"input\"\n:\n \n\"okay\"\n}\n)", "[{'input': 'hand', 'output': 'mano'}]"]}, {"heading": "Use in a Prompt​", "text": "We can now use this example selector in a prompt", "code_blocks": ["from\n langchain_core\n.\nprompts\n.\nfew_shot \nimport\n FewShotPromptTemplate\nfrom\n langchain_core\n.\nprompts\n.\nprompt \nimport\n PromptTemplate\nexample_prompt \n=\n PromptTemplate\n.\nfrom_template\n(\n\"Input: {input} -> Output: {output}\"\n)", "prompt \n=\n FewShotPromptTemplate\n(\n    example_selector\n=\nexample_selector\n,\n    example_prompt\n=\nexample_prompt\n,\n    suffix\n=\n\"Input: {input} -> Output:\"\n,\n    prefix\n=\n\"Translate the following words from English to Italian:\"\n,\n    input_variables\n=\n[\n\"input\"\n]\n,\n)\nprint\n(\nprompt\n.\nformat\n(\ninput\n=\n\"word\"\n)\n)", "Translate the following words from English to Italian:\nInput: hand -> Output: mano\nInput: word -> Output:"]}, {"heading": "Example Selector Types​", "question": "What is covered in the section 'Example Selector Types​'?", "answer": ""}]}
{"title": "How to add a semantic layer over graph database", "url": "https://python.langchain.com/docs/how_to/graph_semantic/", "sections": [{"heading": "Setup​", "text": "First, get required packages and set environment variables: We default to OpenAI models in this guide, but you can swap them out for the model provider of your choice. Next, we need to define Neo4j credentials.\nFollow these installation steps to set up a Neo4j database. The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.", "code_blocks": ["%\npip install \n-\n-\nupgrade \n-\n-\nquiet  langchain langchain\n-\nneo4j langchain\n-\nopenai", "import\n getpass\nimport\n os\nos\n.\nenviron\n[\n\"OPENAI_API_KEY\"\n]\n \n=\n getpass\n.\ngetpass\n(\n)\n# Uncomment the below to use LangSmith. Not required.\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"", " ········", "os\n.\nenviron\n[\n\"NEO4J_URI\"\n]\n \n=\n \n\"bolt://localhost:7687\"\nos\n.\nenviron\n[\n\"NEO4J_USERNAME\"\n]\n \n=\n \n\"neo4j\"\nos\n.\nenviron\n[\n\"NEO4J_PASSWORD\"\n]\n \n=\n \n\"password\"", "from\n langchain_neo4j \nimport\n Neo4jGraph\ngraph \n=\n Neo4jGraph\n(\nrefresh_schema\n=\nFalse\n)\n# Import movie information\nmovies_query \n=\n \n\"\"\"\nLOAD CSV WITH HEADERS FROM \n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\nAS row\nMERGE (m:Movie {id:row.movieId})\nSET m.released = date(row.released),\n    m.title = row.title,\n    m.imdbRating = toFloat(row.imdbRating)\nFOREACH (director in split(row.director, '|') | \n    MERGE (p:Person {name:trim(director)})\n    MERGE (p)-[:DIRECTED]->(m))\nFOREACH (actor in split(row.actors, '|') | \n    MERGE (p:Person {name:trim(actor)})\n    MERGE (p)-[:ACTED_IN]->(m))\nFOREACH (genre in split(row.genres, '|') | \n    MERGE (g:Genre {name:trim(genre)})\n    MERGE (m)-[:IN_GENRE]->(g))\n\"\"\"\ngraph\n.\nquery\n(\nmovies_query\n)", "[]"]}, {"heading": "Custom tools with Cypher templates​", "text": "A semantic layer consists of various tools exposed to an LLM that it can use to interact with a knowledge graph.\nThey can be of various complexity. You can think of each tool in a semantic layer as a function. The function we will implement is to retrieve information about movies or their cast. You can observe that we have defined the Cypher statement used to retrieve information.\nTherefore, we can avoid generating Cypher statements and use the LLM agent to only populate the input parameters.\nTo provide additional information to an LLM agent about when to use the tool and their input parameters, we wrap the function as a tool.", "code_blocks": ["description_query \n=\n \n\"\"\"\nMATCH (m:Movie|Person)\nWHERE m.title CONTAINS $candidate OR m.name CONTAINS $candidate\nMATCH (m)-[r:ACTED_IN|IN_GENRE]-(t)\nWITH m, type(r) as type, collect(coalesce(t.name, t.title)) as names\nWITH m, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as types\nWITH m, collect(types) as contexts\nWITH m, \"type:\" + labels(m)[0] + \"\\ntitle: \"+ coalesce(m.title, m.name) \n       + \"\\nyear: \"+coalesce(m.released,\"\") +\"\\n\" +\n       reduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\") as context\nRETURN context LIMIT 1\n\"\"\"\ndef\n \nget_information\n(\nentity\n:\n \nstr\n)\n \n-\n>\n \nstr\n:\n    \ntry\n:\n        data \n=\n graph\n.\nquery\n(\ndescription_query\n,\n params\n=\n{\n\"candidate\"\n:\n entity\n}\n)\n        \nreturn\n data\n[\n0\n]\n[\n\"context\"\n]\n    \nexcept\n IndexError\n:\n        \nreturn\n \n\"No information was found\"", "from\n typing \nimport\n Optional\n,\n Type\nfrom\n langchain_core\n.\ntools \nimport\n BaseTool\nfrom\n pydantic \nimport\n BaseModel\n,\n Field\nclass\n \nInformationInput\n(\nBaseModel\n)\n:\n    entity\n:\n \nstr\n \n=\n Field\n(\ndescription\n=\n\"movie or a person mentioned in the question\"\n)\nclass\n \nInformationTool\n(\nBaseTool\n)\n:\n    name\n:\n \nstr\n \n=\n \n\"Information\"\n    description\n:\n \nstr\n \n=\n \n(\n        \n\"useful for when you need to answer questions about various actors or movies\"\n    \n)\n    args_schema\n:\n Type\n[\nBaseModel\n]\n \n=\n InformationInput\n    \ndef\n \n_run\n(\n        self\n,\n        entity\n:\n \nstr\n,\n    \n)\n \n-\n>\n \nstr\n:\n        \n\"\"\"Use the tool.\"\"\"\n        \nreturn\n get_information\n(\nentity\n)\n    \nasync\n \ndef\n \n_arun\n(\n        self\n,\n        entity\n:\n \nstr\n,\n    \n)\n \n-\n>\n \nstr\n:\n        \n\"\"\"Use the tool asynchronously.\"\"\"\n        \nreturn\n get_information\n(\nentity\n)"]}, {"heading": "LangGraph Agent​", "question": "What is covered in the section 'LangGraph Agent​'?", "answer": "We will implement a straightforward ReAct agent using LangGraph. The agent consists of an LLM and tools step. As we interact with the agent, we will first call the LLM to decide if we should use tools. Then we will run a loop: If the agent said to take an action (i.e. call tool), we’ll run the tools and pass the results back to the agent.\nIf the agent did not ask to run tools, we will finish (respond to the user). The code implementation is as straightforward as it gets. First we bind the tools to the LLM and define the assistant step. Next we define the LangGraph flow.  Let's test the workflow now with an example question.\n\nfrom\n langchain_core\n.\nmessages \nimport\n HumanMessage\n,\n SystemMessage\nfrom\n langchain_openai \nimport\n ChatOpenAI\nfrom\n langgraph\n.\ngraph \nimport\n MessagesState\nllm \n=\n ChatOpenAI\n(\nmodel\n=\n\"gpt-4o\"\n)\ntools \n=\n \n[\nInformationTool\n(\n)\n]\nllm_with_tools \n=\n llm\n.\nbind_tools\n(\ntools\n)\n# System message\nsys_msg \n=\n SystemMessage\n(\n    content\n=\n\"You are a helpful assistant tasked with finding and explaining relevant information about movies.\"\n)\n# Node\ndef\n \nassistant\n(\nstate\n:\n MessagesState\n)\n:\n    \nreturn\n \n{\n\"messages\"\n:\n \n[\nllm_with_tools\n.\ninvoke\n(\n[\nsys_msg\n]\n \n+\n state\n[\n\"messages\"\n]\n)\n]\n}\n\nfrom\n IPython\n.\ndisplay \nimport\n Image\n,\n display\nfrom\n langgraph\n.\ngraph \nimport\n END\n,\n START\n,\n StateGraph\nfrom\n langgraph\n.\nprebuilt \nimport\n ToolNode\n,\n tools_condition\n# Graph\nbuilder \n=\n StateGraph\n(\nMessagesState\n)\n# Define nodes: these do the work\nbuilder\n.\nadd_node\n(\n\"assistant\"\n,\n assistant\n)\nbuilder\n.\nadd_node\n(\n\"tools\"\n,\n ToolNode\n(\ntools\n)\n)\n# Define edges: these determine how the control flow moves\nbuilder\n.\nadd_edge\n(\nSTART\n,\n \n\"assistant\"\n)\nbuilder\n.\nadd_conditional_edges\n(\n    \n\"assistant\"\n,\n    \n# If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n    \n# If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n    tools_condition\n,\n)\nbuilder\n.\nadd_edge\n(\n\"tools\"\n,\n \n\"assistant\"\n)\nreact_graph \n=\n builder\n.\ncompile\n(\n)\n# Show\ndisplay\n(\nImage\n(\nreact_graph\n.\nget_graph\n(\nxray\n=\nTrue\n)\n.\ndraw_mermaid_png\n(\n)\n)\n)\n\ninput_messages \n=\n \n[\nHumanMessage\n(\ncontent\n=\n\"Who played in the Casino?\"\n)\n]\nmessages \n=\n react_graph\n.\ninvoke\n(\n{\n\"messages\"\n:\n input_messages\n}\n)\nfor\n m \nin\n messages\n[\n\"messages\"\n]\n:\n    m\n.\npretty_print\n(\n)\n\n================================\u001b[1m Human Message \u001b[0m=================================\nWho played in the Casino?\n==================================\u001b[1m Ai Message \u001b[0m==================================\nTool Calls:\n  Information (call_j4usgFStGtBM16fuguRaeoGc)\n Call ID: call_j4usgFStGtBM16fuguRaeoGc\n  Args:\n    entity: Casino\n=================================\u001b[1m Tool Message \u001b[0m=================================\nName: Information\ntype:Movie\ntitle: Casino\nyear: 1995-11-22\nACTED_IN: Robert De Niro, Joe Pesci, Sharon Stone, James Woods\nIN_GENRE: Drama, Crime\n==================================\u001b[1m Ai Message \u001b[0m==================================\nThe movie \"Casino,\" released in 1995, features the following actors:\n- Robert De Niro\n- Joe Pesci\n- Sharon Stone\n- James Woods\nThe film is in the Drama and Crime genres."}]}
{"title": "How to invoke runnables in parallel", "url": "https://python.langchain.com/docs/how_to/parallel/", "sections": [{"heading": "Formatting with RunnableParallels​", "text": "RunnableParallels are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following: Below, the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key. Note that when composing a RunnableParallel with another Runnable we don't even need to wrap our dictionary in the RunnableParallel class — the type conversion is handled for us. In the context of a chain, these are equivalent: See the section on coercion for more.", "code_blocks": ["     Input\n      / \\\n     /   \\\n Branch1 Branch2\n     \\   /\n      \\ /\n      Combine", "from\n langchain_community\n.\nvectorstores \nimport\n FAISS\nfrom\n langchain_core\n.\noutput_parsers \nimport\n StrOutputParser\nfrom\n langchain_core\n.\nprompts \nimport\n ChatPromptTemplate\nfrom\n langchain_core\n.\nrunnables \nimport\n RunnablePassthrough\nfrom\n langchain_openai \nimport\n ChatOpenAI\n,\n OpenAIEmbeddings\nvectorstore \n=\n FAISS\n.\nfrom_texts\n(\n    \n[\n\"harrison worked at kensho\"\n]\n,\n embedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever \n=\n vectorstore\n.\nas_retriever\n(\n)\ntemplate \n=\n \n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\n# The prompt expects input with keys for \"context\" and \"question\"\nprompt \n=\n ChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nmodel \n=\n ChatOpenAI\n(\n)\nretrieval_chain \n=\n \n(\n    \n{\n\"context\"\n:\n retriever\n,\n \n\"question\"\n:\n RunnablePassthrough\n(\n)\n}\n    \n|\n prompt\n    \n|\n model\n    \n|\n StrOutputParser\n(\n)\n)\nretrieval_chain\n.\ninvoke\n(\n\"where did harrison work?\"\n)", "'Harrison worked at Kensho.'", "{\"context\": retriever, \"question\": RunnablePassthrough()}", "RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})", "RunnableParallel(context=retriever, question=RunnablePassthrough())"]}, {"heading": "Using itemgetter as shorthand​", "text": "Note that you can use Python's itemgetter as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about itemgetter in the Python Documentation. In the example below, we use itemgetter to extract specific keys from the map:", "code_blocks": ["from\n operator \nimport\n itemgetter\nfrom\n langchain_community\n.\nvectorstores \nimport\n FAISS\nfrom\n langchain_core\n.\noutput_parsers \nimport\n StrOutputParser\nfrom\n langchain_core\n.\nprompts \nimport\n ChatPromptTemplate\nfrom\n langchain_core\n.\nrunnables \nimport\n RunnablePassthrough\nfrom\n langchain_openai \nimport\n ChatOpenAI\n,\n OpenAIEmbeddings\nvectorstore \n=\n FAISS\n.\nfrom_texts\n(\n    \n[\n\"harrison worked at kensho\"\n]\n,\n embedding\n=\nOpenAIEmbeddings\n(\n)\n)\nretriever \n=\n vectorstore\n.\nas_retriever\n(\n)\ntemplate \n=\n \n\"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\nAnswer in the following language: {language}\n\"\"\"\nprompt \n=\n ChatPromptTemplate\n.\nfrom_template\n(\ntemplate\n)\nchain \n=\n \n(\n    \n{\n        \n\"context\"\n:\n itemgetter\n(\n\"question\"\n)\n \n|\n retriever\n,\n        \n\"question\"\n:\n itemgetter\n(\n\"question\"\n)\n,\n        \n\"language\"\n:\n itemgetter\n(\n\"language\"\n)\n,\n    \n}\n    \n|\n prompt\n    \n|\n model\n    \n|\n StrOutputParser\n(\n)\n)\nchain\n.\ninvoke\n(\n{\n\"question\"\n:\n \n\"where did harrison work\"\n,\n \n\"language\"\n:\n \n\"italian\"\n}\n)", "'Harrison ha lavorato a Kensho.'"]}, {"heading": "Parallelize steps​", "text": "RunnableParallels make it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.", "code_blocks": ["from\n langchain_core\n.\nprompts \nimport\n ChatPromptTemplate\nfrom\n langchain_core\n.\nrunnables \nimport\n RunnableParallel\nfrom\n langchain_openai \nimport\n ChatOpenAI\nmodel \n=\n ChatOpenAI\n(\n)\njoke_chain \n=\n ChatPromptTemplate\n.\nfrom_template\n(\n\"tell me a joke about {topic}\"\n)\n \n|\n model\npoem_chain \n=\n \n(\n    ChatPromptTemplate\n.\nfrom_template\n(\n\"write a 2-line poem about {topic}\"\n)\n \n|\n model\n)\nmap_chain \n=\n RunnableParallel\n(\njoke\n=\njoke_chain\n,\n poem\n=\npoem_chain\n)\nmap_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n \n\"bear\"\n}\n)", "{'joke': AIMessage(content=\"Why don't bears like fast food? Because they can't catch it!\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe024170-c251-4b7a-bfd4-64a3737c67f2-0'),\n 'poem': AIMessage(content='In the quiet of the forest, the bear roams free\\nMajestic and wild, a sight to see.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2707913e-a743-4101-b6ec-840df4568a76-0')}"]}, {"heading": "Parallelism​", "text": "RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.", "code_blocks": ["%\n%\ntimeit\njoke_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n \n\"bear\"\n}\n)", "610 ms ± 64 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)", "%\n%\ntimeit\npoem_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n \n\"bear\"\n}\n)", "599 ms ± 73.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)", "%\n%\ntimeit\nmap_chain\n.\ninvoke\n(\n{\n\"topic\"\n:\n \n\"bear\"\n}\n)", "643 ms ± 77.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"]}, {"heading": "Next steps​", "question": "What is covered in the section 'Next steps​'?", "answer": "You now know some ways to format and parallelize chain steps with RunnableParallel. To learn more, see the other how-to guides on runnables in this section."}]}
{"title": "How to stream chat model responses", "url": "https://python.langchain.com/docs/how_to/chat_streaming/", "sections": [{"heading": "Sync streaming​", "text": "Below we use a | to help visualize the delimiter between tokens.", "code_blocks": ["from\n langchain_anthropic\n.\nchat_models \nimport\n ChatAnthropic\nchat \n=\n ChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nfor\n chunk \nin\n chat\n.\nstream\n(\n\"Write me a 1 verse song about goldfish on the moon\"\n)\n:\n    \nprint\n(\nchunk\n.\ncontent\n,\n end\n=\n\"|\"\n,\n flush\n=\nTrue\n)", "Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|\nFloating| up| in| the| star|ry| night|,|\nFins| a|-|gl|im|mer| in| the| pale| moon|light|.|\nGol|dfish| swimming|,| peaceful| an|d free|,|\nSe|ren|ely| |drif|ting| across| the| lunar| sea|.|"]}, {"heading": "Async Streaming​", "text": "", "code_blocks": ["from\n langchain_anthropic\n.\nchat_models \nimport\n ChatAnthropic\nchat \n=\n ChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nasync\n \nfor\n chunk \nin\n chat\n.\nastream\n(\n\"Write me a 1 verse song about goldfish on the moon\"\n)\n:\n    \nprint\n(\nchunk\n.\ncontent\n,\n end\n=\n\"|\"\n,\n flush\n=\nTrue\n)", "Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|\nFloating| up| above| the| Earth|,|\nGol|dfish| swim| in| alien| m|irth|.|\nIn| their| bowl| of| lunar| dust|,|\nGl|it|tering| scales| reflect| the| trust|\nOf| swimming| free| in| this| new| worl|d,|\nWhere| their| aqu|atic| dream|'s| unf|ur|le|d.|"]}, {"heading": "Astream events​", "question": "What is covered in the section 'Astream events​'?", "answer": "Chat models also support the standard astream events method. This method is useful if you're streaming output from a larger LLM application that contains multiple steps (e.g., an LLM chain composed of a prompt, llm and parser).\n\nfrom\n langchain_anthropic\n.\nchat_models \nimport\n ChatAnthropic\nchat \n=\n ChatAnthropic\n(\nmodel\n=\n\"claude-3-haiku-20240307\"\n)\nidx \n=\n \n0\nasync\n \nfor\n event \nin\n chat\n.\nastream_events\n(\n    \n\"Write me a 1 verse song about goldfish on the moon\"\n)\n:\n    idx \n+=\n \n1\n    \nif\n idx \n>=\n \n5\n:\n  \n# Truncate the output\n        \nprint\n(\n\"...Truncated\"\n)\n        \nbreak\n    \nprint\n(\nevent\n)\n\n{'event': 'on_chat_model_start', 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'name': 'ChatAnthropic', 'tags': [], 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e', usage_metadata={'input_tokens': 21, 'output_tokens': 2, 'total_tokens': 23, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=\"Here's\", additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}\n{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' a short one-verse song', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}\n...Truncated"}]}
